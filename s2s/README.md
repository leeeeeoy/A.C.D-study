# Sequence-to-Sequence, seq2seq

## Sequence-to-Sequence??

- 번역기에서 대표적으로 사용되는 모델
- 인코더와 디코더로 구성

### Context vector

: 인코더에서 입력 문장의 모든 단어들을 압축해서 만든 벡터

- 입력 문장의 정보가 하나의 벡터로 압축
- 디코더로 벡터를 전송 후, 순차적으로 출력
- 보통 수백 이상의 차원을 가짐

### Encoder

: 입력 문장을 받는 RNN 셀

- 모든 단어를 입력받은 뒤 RNN 셀의 마지막 시점의 은닉상태를 넘겨줌 -> 컨텍스트 벡터
- 컨텍스트 벡터는 디코더 RNN 셀의 첫번째 은닉 상태로 사용

### Decoder

: 출력 문장을 출력하는 RNN 셀

- 초기 입력으로 문장의 시작을 의미하는 \<sos>심볼이 들어감
- 그 후 다음에 등장할 확률이 높은 단어를 예측
- 예측한 단어를 다음 RNN 셀의 입력으로 보냄
- \<eos> 심볼을 받을 때까지 반복해서 예측
