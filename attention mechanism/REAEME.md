# Attention Mechanism

: 특정 벡터에 주목(attention)하게 만들어 성능을 높이는 기법

## Basic idea

- 하나의 고정된 크기의 벡터에서 발생하는 정보 손실 감소 목표
- RNN의 고질적 문제인 Vanishing Gradient 문제 해결

#### Vanishing Gradient(기울기 소실)

- 역전파 과정에서 입력층으로 갈수록 기울기가 점차 작아지는 현상 발생
- 입력층에서 가까운 층들에서 가중치들이 제대로 업데이트가 되지 않아 최적의 모델을 찾을 수 없는 문제

#### 기존 S2S 모델의 한계

- 인코더는 입력 시퀀스를 하나의 벡터로 압출
- 디코더는 이 벡터를 통해 출력 시퀀스 생성
- 이 과정에서 입력 시퀀스의 정보의 손실 발생

## Attention function

- Q(query): 주어진 쿼리, t시점의 디코더 셀에서의 은닉 상태들
- K(key): 키, 모든 시점의 인코더 셀의 은닉 상태들
- V(value): 값, 모든 시점의 인코더 셀의 은닉 상태들

## Basic mechanism

- 디코더는 다음 출력을 예측할 때 인코더의 입력 정보를 확인하려고 함
- 인코더의 소프트맥스 함수에서 각 입력에 대한 도움 정도를 수치화 함
- 수치화된 정보를 하나의 정보로 담아 디코더로 전송
- 디코더는 이 정보를 통해 출력 단어를 보다 더 정확하게 예측

## 기타 고려해 볼 점들

### 단어 수와 학습결과

- 한글 데이터는 영어 데이터에 비해 비교적 많은 단어 수 포함
- 같은 양의 데이터를 가정했을 때, 전체 단어의 수 증가
- 일반적인 단어로 기준을 제한했을 때, 제외되는 단어의 수 증가 or 예측해야 하는 단어의 수 증가
- 현재 학습 단계에서는 단어 기준을 제한 후, 평가 단계에서 해당 단어와 유사한 단어의 범위를 늘리는 방안 사용<br/>
  --> 해당 유의어 사전도 위치 기반으로 형성된 데이터, 의미적으로 유사하지 않은 경우 종종 발생

### 데이터 자체에 대한 언어 처리

- 현재 형태소 분석을 이용한 전처리 단계 적용 중
- 특정 기준을 중심으로 단어 집합 형성이 가능한가??
- 사전적 동의어/다의어 등을 사용할 수 있는 방안 고려

### 생성되는 요약의 한계

- 지도학습 기반
- 보편적으로 학습이 잘 되었다면 해당 데이터 안의 단어들의 조합을 통해 요약 생성
- 즉 데이터에 존재하지 않는 외부 단어를 조합시킨 예측은 기대하기 힘듬
- 추출요약과 완전 생성 요약 그 중간 사이의 단계정도...??
