# BERT

: Bidirectional Encoder Representations from Transformers<br/>
: 대부분의 NLP 분야에서 좋은 성능을 보여주고 있는 모델

## Basic idea

- 레이블이 없는 많은 데이터로 사전 훈련
- 레이블이 있는 추가 훈련을 통한 하이퍼파라미터 조절
